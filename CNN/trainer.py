from __future__ import absolute_import, division, print_functionimport numpy as npimport timeimport torchimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils.data import DataLoaderfrom tensorboardX import SummaryWriterimport jsonfrom utils import *from kitti_utils import *from layers import *import datasetsimport networksfrom cat import ChannelAwareTransformer, ChannelAwareTransformer2from IPython import embedfrom thop import clever_format, profilefrom linear_warmup_cosine_annealing_warm_restarts_weight_decay import ChainedSchedulerimport matplotlib.pyplot as pltimport matplotlibtorch.backends.cudnn.benchmark = Truedef profile_once(encoder, x):    x_e = x[0, :, :, :].unsqueeze(0)    # input(x_e.shape)    x_d, _ = encoder.profile_encoder(x_e)    flops_e, params_e = profile(encoder, inputs=(x_e, ), verbose=False)    flops_d, params_d = profile(encoder.decoder, inputs=(x_d, ), verbose=False)    flops, params = clever_format([flops_e+flops_d, params_e+params_d], "%.3f")    flops_e, params_e = clever_format([flops_e, params_e], "%.3f")    flops_d, params_d = clever_format([flops_d, params_d], "%.3f")    return flops, params, flops_e, params_e, flops_d, params_ddef time_sync():    # PyTorch-accurate time    if torch.cuda.is_available():        torch.cuda.synchronize()    return time.time()class Trainer:    def __init__(self, options):        self.opt = options        self.log_path = os.path.join(self.opt.log_dir, self.opt.model_name)        # checking height and width are multiples of 32        assert self.opt.height % 32 == 0, "'height' must be a multiple of 32"        assert self.opt.width % 32 == 0, "'width' must be a multiple of 32"        self.models = {}        self.models_pose = {}        # self.models_mlp = {}        self.parameters_to_train = []        self.parameters_to_train_pose = []        self.parameters_to_train_mlp = []        self.device = torch.device("cpu" if self.opt.no_cuda else "cuda")        self.profile = self.opt.profile        self.num_scales = len(self.opt.scales)        self.frame_ids = len(self.opt.frame_ids)        self.num_pose_frames = 2 if self.opt.pose_model_input == "pairs" else self.num_input_frames        assert self.opt.frame_ids[0] == 0, "frame_ids must start with 0"        self.use_pose_net = not (self.opt.use_stereo and self.opt.frame_ids == [0])        if self.opt.use_stereo:            self.opt.frame_ids.append("s")        if self.opt.model == "drone-mono":            self.models["encoder"] = networks.DroneMono2()            # self.models["encoder"] = networks.DroneMono2_onnx(onnx=True)        else:            self.models["encoder"] = networks.LiteMono(model='lite-mono')        self.models["encoder"].to(self.device)        self.parameters_to_train += list(self.models["encoder"].parameters())        if self.models['encoder']._get_name() == 'LiteMono':            self.models["depth"] = networks.DroneDepthDecoder(                self.models["encoder"].num_ch_enc, self.opt.scales)            self.models["depth"].to(self.device)            self.parameters_to_train += list(self.models["depth"].parameters())        if len(self.opt.distill) > 0:            setattr(self.opt, 'models_to_load', ["pose_encoder", "pose"])            # setattr(self.opt, 'load_weights_folder', 'models/mono_640x192/')            # setattr(self.opt, 'load_weights_folder', 'tmp/mono_gray_pt/models/weights_22/')            setattr(self.opt, 'load_weights_folder', 'tmp/mono_drone_litemono4/models/weights_199/')            self.teacher = networks.LiteMono()            self.teacher_depth = networks.DepthDecoder(                self.teacher.num_ch_enc, range(3))            self.teacher.to(self.device)            self.teacher_depth.to(self.device)            if len(self.opt.distill) == 1 and "dec_feat" in self.opt.distill:                self.distill_mode = 0                # self.models["teacher_mlp"] = networks.DistillN(self.teacher_depth.num_ch_dec,                #                                                     self.models["depth"].num_ch_dec)                self.models["align_mlp"] = networks.DistillN(self.models["encoder"].decoder.num_ch_dec, self.teacher_depth.num_ch_dec)                # self.models["align_mlp"] = networks.DistillD(self.models["depth"].num_ch_dec)                self.models["align_mlp"].to(self.device)                self.parameters_to_train_mlp += list(self.models["align_mlp"].parameters())                self.cat = ChannelAwareTransformer2(self.teacher_depth.num_ch_dec.tolist()).to(self.device)                self.parameters_to_train_mlp += list(self.cat.parameters())            elif len(self.opt.distill) == 1 and "enc_feat" in self.opt.distill:                self.distill_mode = 1                s_enc = np.concatenate(([self.models["encoder"].num_ch_enc[0]], self.models["encoder"].num_ch_enc))                t_enc = np.concatenate(([self.teacher.num_ch_enc[0]], self.teacher.num_ch_enc))                self.models["align_mlp"] = networks.DistillN(s_enc, t_enc)                self.models["align_mlp"].to(self.device)                self.parameters_to_train_mlp += list(self.models["align_mlp"].parameters())                self.cat = ChannelAwareTransformer2([self.teacher.num_ch_enc[0]]+self.teacher.num_ch_enc.tolist()).to(self.device)                self.parameters_to_train_mlp += list(self.cat.parameters())            elif len(self.opt.distill) == 2 and "enc_feat" in self.opt.distill and "dec_feat" in self.opt.distill:                self.distill_mode = 2                # s_enc = self.models["encoder"].num_ch_enc                # t_enc = self.teacher.num_ch_enc                s_enc = np.concatenate(([self.models["encoder"].num_ch_enc[0]], self.models["encoder"].num_ch_enc))                t_enc = np.concatenate(([self.teacher.num_ch_enc[0]], self.teacher.num_ch_enc))                self.models["align_mlp_enc"] = networks.DistillN(s_enc, t_enc)                self.models["align_mlp_enc"].to(self.device)                self.parameters_to_train_mlp += list(self.models["align_mlp_enc"].parameters())                self.models["align_mlp_dec"] = networks.DistillN(self.models["encoder"].decoder.num_ch_dec, self.teacher_depth.num_ch_dec)                self.models["align_mlp_dec"].to(self.device)                self.parameters_to_train_mlp += list(self.models["align_mlp_dec"].parameters())                self.cat = ChannelAwareTransformer2([self.teacher.num_ch_enc[0]]+self.teacher.num_ch_enc.tolist()+self.teacher_depth.num_ch_dec.tolist()).to(self.device)                self.parameters_to_train_mlp += list(self.cat.parameters())        if self.use_pose_net:            if self.opt.pose_model_type == "separate_resnet":                self.models_pose["pose_encoder"] = networks.ResnetEncoder(                    self.opt.num_layers,                    self.opt.weights_init == "pretrained",                    num_input_images=self.num_pose_frames)                self.models_pose["pose_encoder"].to(self.device)                # self.parameters_to_train += list(self.models["pose_encoder"].parameters())                self.parameters_to_train_pose += list(self.models_pose["pose_encoder"].parameters())                self.models_pose["pose"] = networks.PoseDecoder(                    self.models_pose["pose_encoder"].num_ch_enc,                    num_input_features=1,                    num_frames_to_predict_for=2)            elif self.opt.pose_model_type == "shared":                self.models_pose["pose"] = networks.PoseDecoder(                    self.models["encoder"].num_ch_enc, self.num_pose_frames)            elif self.opt.pose_model_type == "posecnn":                self.models_pose["pose"] = networks.PoseCNN(                    self.num_input_frames if self.opt.pose_model_input == "all" else 2)            self.models_pose["pose"].to(self.device)            self.parameters_to_train_pose += list(self.models_pose["pose"].parameters())        if self.opt.predictive_mask:            assert self.opt.disable_automasking, \                "When using predictive_mask, please disable automasking with --disable_automasking"            # Our implementation of the predictive masking baseline has the the same architecture            # as our depth decoder. We predict a separate mask for each source frame.            self.models["predictive_mask"] = networks.DepthDecoder(                self.models["encoder"].num_ch_enc, self.opt.scales,                num_output_channels=(len(self.opt.frame_ids) - 1))            self.models["predictive_mask"].to(self.device)            self.parameters_to_train += list(self.models["predictive_mask"].parameters())        self.model_optimizer = optim.AdamW(self.parameters_to_train, self.opt.lr[0], weight_decay=1e-2)        if self.use_pose_net:            self.model_pose_optimizer = optim.AdamW(self.parameters_to_train_pose, self.opt.lr[3], weight_decay=1e-2)        if len(self.opt.distill) > 0:            self.model_mlp_optimizer = optim.AdamW(self.parameters_to_train_mlp, 0.0002, weight_decay=1e-2)        # self.model_lr_scheduler = optim.lr_scheduler.StepLR(        #     self.model_optimizer, self.opt.scheduler_step_size, 0.1)        if True:            self.model_lr_scheduler = ChainedScheduler(                self.model_optimizer,                T_0=int(self.opt.lr[2]),                T_mul=1,                eta_min=self.opt.lr[1],                last_epoch=-1,                max_lr=self.opt.lr[0],                warmup_steps=0,                gamma=0.9            )            if self.use_pose_net:                self.model_pose_lr_scheduler = ChainedScheduler(                    self.model_pose_optimizer,                    T_0=int(self.opt.lr[5]),                    T_mul=1,                    eta_min=self.opt.lr[4],                    last_epoch=-1,                    max_lr=self.opt.lr[3],                    warmup_steps=0,                    gamma=0.9                )            if len(self.opt.distill) >0:                self.model_mlp_lr_scheduler = ChainedScheduler(                    self.model_mlp_optimizer,                    T_0=100,                    T_mul=1,                    eta_min=0.000001,                    last_epoch=-1,                    max_lr=0.0002,                    warmup_steps=0,                    gamma=0.9)                        if self.opt.load_weights_folder is not None:            self.load_model()        if self.opt.mypretrain is not None:            self.load_pretrain()                print("Training model named:\n  ", self.opt.model_name)        print("Models and tensorboard events files are saved to:\n  ", self.opt.log_dir)        print("Training is using:\n  ", self.device)        # data        datasets_dict = {"kitti": datasets.KITTIRAWDataset,                         "kitti_odom": datasets.KITTIOdomDataset,                         "drone": datasets.DRONEDataset}        self.dataset = datasets_dict[self.opt.dataset]        fpath = os.path.join(os.path.dirname(__file__), "splits", self.opt.split, "{}_files.txt")        train_filenames = readlines(fpath.format("train"))        val_filenames = readlines(fpath.format("val"))        img_ext = '.png' if self.opt.png else '.jpg'        num_train_samples = len(train_filenames)        self.num_total_steps = num_train_samples // self.opt.batch_size * self.opt.num_epochs        train_dataset = self.dataset(            self.opt.data_path, train_filenames, self.opt.height, self.opt.width,            self.opt.frame_ids, 4, is_train=True, img_ext=img_ext)        self.train_loader = DataLoader(            train_dataset, self.opt.batch_size, True,            num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)        val_dataset = self.dataset(            self.opt.data_path, val_filenames, self.opt.height, self.opt.width,            self.opt.frame_ids, 4, is_train=False, img_ext=img_ext)        self.val_loader = DataLoader(            val_dataset, self.opt.batch_size, True,            num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)        self.val_iter = iter(self.val_loader)        self.writers = {}        for mode in ["train", "val"]:            self.writers[mode] = SummaryWriter(os.path.join(self.log_path, mode))        if not self.opt.no_ssim:            self.ssim = SSIM()            self.ssim.to(self.device)        self.backproject_depth = {}        self.project_3d = {}        for scale in self.opt.scales:            h = self.opt.height // (2 ** scale)            w = self.opt.width // (2 ** scale)            self.backproject_depth[scale] = BackprojectDepth(self.opt.batch_size, h, w)            self.backproject_depth[scale].to(self.device)            self.project_3d[scale] = Project3D(self.opt.batch_size, h, w)            self.project_3d[scale].to(self.device)        self.depth_metric_names = [            "de/abs_rel", "de/sq_rel", "de/rms", "de/log_rms", "da/a1", "da/a2", "da/a3"]        print("Using split:\n  ", self.opt.split)        print("There are {:d} training items and {:d} validation items\n".format(            len(train_dataset), len(val_dataset)))        self.save_opts()    def set_train(self):        """Convert all models to training mode        """        for m in self.models.values():            m.train()    def set_eval(self):        """Convert all models to testing/evaluation mode        """        for m in self.models.values():            m.eval()    def train(self):        """Run the entire training pipeline        """        self.epoch = 0        self.step = 0        self.start_time = time.time()        for self.epoch in range(self.opt.num_epochs):            self.run_epoch()            if (self.epoch + 1) % self.opt.save_frequency == 0:                self.save_model()    def run_epoch(self):        """Run a single epoch of training and validation        """        print("Training")        self.set_train()        self.model_lr_scheduler.step()        if self.use_pose_net:            self.model_pose_lr_scheduler.step()        if len(self.opt.distill) >0:            self.model_mlp_lr_scheduler.step()        for batch_idx, inputs in enumerate(self.train_loader):            before_op_time = time.time()            outputs, losses, teacher_outputs = self.process_batch(inputs)            self.model_optimizer.zero_grad()            if self.use_pose_net:                self.model_pose_optimizer.zero_grad()            if len(self.opt.distill) > 0:                self.model_mlp_optimizer.zero_grad()            losses["loss"].backward()            self.model_optimizer.step()            if self.use_pose_net:                self.model_pose_optimizer.step()            if len(self.opt.distill) > 0:                self.model_mlp_optimizer.step()            duration = time.time() - before_op_time            # log less frequently after the first 2000 steps to save time & disk space            early_phase = batch_idx % self.opt.log_frequency == 0 and self.step < 2000            late_phase = self.step % 6000 == 0            if early_phase or late_phase:                self.log_time(batch_idx, duration, losses["loss"].cpu().data)                if "depth_gt" in inputs:                    self.compute_depth_losses(inputs, outputs, losses)                self.log("train", inputs, outputs, losses, teacher_outputs)                self.val()            self.step += 1    def process_batch(self, inputs):        """Pass a minibatch through the network and generate images and losses        """        for key, ipt in inputs.items():            inputs[key] = ipt.to(self.device)        if self.opt.pose_model_type == "shared":            # If we are using a shared encoder for both depth and pose (as advocated            # in monodepthv1), then all images are fed separately through the depth encoder.            all_color_aug = torch.cat([inputs[("color_aug", i, 0)] for i in self.opt.frame_ids])            all_features = self.models["encoder"](all_color_aug)            all_features = [torch.split(f, self.opt.batch_size) for f in all_features]            features = {}            for i, k in enumerate(self.opt.frame_ids):                features[k] = [f[i] for f in all_features]            outputs = self.models["depth"](features[0])        else:            # input(inputs["color_aug", 0, 0].shape)            # Otherwise, we only feed the image with frame_id 0 through the depth encoder            if self.profile:                self.set_eval()                flops, params, flops_e, params_e, flops_d, params_d = profile_once(self.models["encoder"], inputs["color_aug", 0, 0])                self.profile = False                self.set_train()                print("\n  " + ("flops: {0}, params: {1}, flops_e: {2}, params_e:{3}, flops_d:{4}, params_d:{5}").format(flops, params, flops_e, params_e, flops_d, params_d))            if len(self.opt.distill) > 0:                features, features_distill, outputs, s_dis_dec = self.models["encoder"](inputs["color_aug", 0, 0], distill=True)            else:                if self.models['encoder']._get_name() =='LiteMono':                    features = self.models["encoder"](inputs["color_aug", 0, 0], distill=False)                    outputs, dis_features = self.models["depth"](features)                else:                    features, features_distill, outputs, s_dis_dec = self.models["encoder"](inputs["color_aug", 0, 0], distill=True)                    # features = self.models["encoder"](inputs["color_aug", 0, 0], distill=True)                        if len(self.opt.distill) == 1 and "enc_feat" in self.opt.distill:                with torch.no_grad():                    t_enc_out, t_dis_enc = self.teacher(inputs["color_aug", 0, 0])                    t_dec_out, t_dis_dec = self.teacher_depth(t_enc_out)                features_teacher = t_dis_enc                features_student = self.models["align_mlp"](features_distill)                teacher_outputs = t_dec_out            elif len(self.opt.distill) == 1 and "dec_feat" in self.opt.distill:                features_teacher = []                features_student = []                # input(self.models['depth'].num_ch_dec)                with torch.no_grad():                    t_enc_out, t_dis_enc = self.teacher(inputs["color_aug", 0, 0])                    t_dec_out, t_dis_dec = self.teacher_depth(t_enc_out)                    for i in range(len(t_dis_dec)-1, -1, -1):                        features_teacher.append(t_dis_dec[i])                # features_teacher = self.models["teacher_mlp"](features_teacher)                for i in range(len(s_dis_dec)-1, -1, -1):                    features_student.append(s_dis_dec[i])                features_student = self.models["align_mlp"](features_student)                                teacher_outputs = t_dec_out            elif len(self.opt.distill) == 2:                features_teacher_dec = []                features_student_dec = []                with torch.no_grad():                    t_enc_out, t_dis_enc = self.teacher(inputs["color_aug", 0, 0])                    t_dec_out, t_dis_dec = self.teacher_depth(t_enc_out)                    for i in range(len(t_dis_dec) - 1, -1, -1):                        features_teacher_dec.append(t_dis_dec[i])                # for i in range(len(t_dis_enc)):                #     print(t_dis_enc[i].shape, t_dis_dec[i].shape)                for i in range(len(s_dis_dec)-1, -1, -1):                    features_student_dec.append(s_dis_dec[i])                features_student_enc = self.models["align_mlp_enc"](features_distill)                features_student_dec = self.models["align_mlp_dec"](features_student_dec)                features_teacher = t_dis_enc+features_teacher_dec                features_student = features_student_enc+features_student_dec                teacher_outputs = t_dec_out            else:                features_teacher = None                features_student = None                teacher_outputs = None        if self.opt.predictive_mask:            outputs["predictive_mask"] = self.models["predictive_mask"](features)        if self.use_pose_net:            outputs.update(self.predict_poses(inputs, features))        self.generate_images_pred(inputs, outputs)        losses = self.compute_losses(inputs, outputs,                                     features=features_student,                                     features_teacher=features_teacher,                                     teacher_outputs=teacher_outputs)        return outputs, losses, teacher_outputs    def predict_poses(self, inputs, features):        """Predict poses between input frames for monocular sequences.        """        outputs = {}        if self.num_pose_frames == 2:            # In this setting, we compute the pose to each source frame via a            # separate forward pass through the pose network.            # select what features the pose network takes as input            if self.opt.pose_model_type == "shared":                pose_feats = {f_i: features[f_i] for f_i in self.opt.frame_ids}            else:                pose_feats = {f_i: inputs["color_aug", f_i, 0] for f_i in self.opt.frame_ids}            for f_i in self.opt.frame_ids[1:]:                if f_i != "s":                    # To maintain ordering we always pass frames in temporal order                    if f_i < 0:                        pose_inputs = [pose_feats[f_i], pose_feats[0]]                    else:                        pose_inputs = [pose_feats[0], pose_feats[f_i]]                    if self.opt.pose_model_type == "separate_resnet":                        # with torch.no_grad():                        pose_inputs = [self.models_pose["pose_encoder"](torch.cat(pose_inputs, 1))]                    elif self.opt.pose_model_type == "posecnn":                        pose_inputs = torch.cat(pose_inputs, 1)                    # with torch.no_grad():                    axisangle, translation = self.models_pose["pose"](pose_inputs)                    outputs[("axisangle", 0, f_i)] = axisangle                    outputs[("translation", 0, f_i)] = translation                    # Invert the matrix if the frame id is negative                    outputs[("cam_T_cam", 0, f_i)] = transformation_from_parameters(                        axisangle[:, 0], translation[:, 0], invert=(f_i < 0))        else:            # Here we input all frames to the pose net (and predict all poses) together            if self.opt.pose_model_type in ["separate_resnet", "posecnn"]:                pose_inputs = torch.cat(                    [inputs[("color_aug", i, 0)] for i in self.opt.frame_ids if i != "s"], 1)                if self.opt.pose_model_type == "separate_resnet":                    pose_inputs = [self.models["pose_encoder"](pose_inputs)]            elif self.opt.pose_model_type == "shared":                pose_inputs = [features[i] for i in self.opt.frame_ids if i != "s"]            # with torch.no_grad():            axisangle, translation = self.models_pose["pose"](pose_inputs)            for i, f_i in enumerate(self.opt.frame_ids[1:]):                if f_i != "s":                    outputs[("axisangle", 0, f_i)] = axisangle                    outputs[("translation", 0, f_i)] = translation                    outputs[("cam_T_cam", 0, f_i)] = transformation_from_parameters(                        axisangle[:, i], translation[:, i])        return outputs    def val(self):        """Validate the model on a single minibatch        """        self.set_eval()        try:            inputs = self.val_iter.next()        except StopIteration:            self.val_iter = iter(self.val_loader)            inputs = self.val_iter.next()        with torch.no_grad():            outputs, losses, teacher_outputs = self.process_batch(inputs)            if "depth_gt" in inputs:                self.compute_depth_losses(inputs, outputs, losses)            self.log("val", inputs, outputs, losses, teacher_outputs)            del inputs, outputs, losses        self.set_train()    def generate_images_pred(self, inputs, outputs):        """Generate the warped (reprojected) color images for a minibatch.        Generated images are saved into the `outputs` dictionary.        """        for scale in self.opt.scales:            disp = outputs[("disp", scale)]            if self.opt.v1_multiscale:                source_scale = scale            else:                disp = F.interpolate(                    disp, [self.opt.height, self.opt.width], mode="bilinear", align_corners=False)                source_scale = 0            _, depth = disp_to_depth(disp, self.opt.min_depth, self.opt.max_depth)            outputs[("depth", 0, scale)] = depth            for i, frame_id in enumerate(self.opt.frame_ids[1:]):                if frame_id == "s":                    T = inputs["stereo_T"]                else:                    T = outputs[("cam_T_cam", 0, frame_id)]                # from the authors of https://arxiv.org/abs/1712.00175                if self.opt.pose_model_type == "posecnn":                    axisangle = outputs[("axisangle", 0, frame_id)]                    translation = outputs[("translation", 0, frame_id)]                    inv_depth = 1 / depth                    mean_inv_depth = inv_depth.mean(3, True).mean(2, True)                    T = transformation_from_parameters(                        axisangle[:, 0], translation[:, 0] * mean_inv_depth[:, 0], frame_id < 0)                cam_points = self.backproject_depth[source_scale](                    depth, inputs[("inv_K", source_scale)])                pix_coords = self.project_3d[source_scale](                    cam_points, inputs[("K", source_scale)], T)                outputs[("sample", frame_id, scale)] = pix_coords                outputs[("color", frame_id, scale)] = F.grid_sample(                    inputs[("color", frame_id, source_scale)],                    outputs[("sample", frame_id, scale)],                    padding_mode="border", align_corners=True)                if not self.opt.disable_automasking:                    outputs[("color_identity", frame_id, scale)] = \                        inputs[("color", frame_id, source_scale)]    def feature_loss_function(self, fea, target_fea):        loss = (fea - target_fea) ** 2 * ((fea > 0) | (target_fea > 0)).float()        return torch.abs(loss).mean()    def compute_reprojection_loss(self, pred, target):        """Computes reprojection loss between a batch of predicted and target images        """        abs_diff = torch.abs(target - pred)        l1_loss = abs_diff.mean(1, True)        if self.opt.no_ssim:            reprojection_loss = l1_loss        else:            ssim_loss = self.ssim(pred, target).mean(1, True)            reprojection_loss = 1*(0.85 * ssim_loss + 0.15 * l1_loss)        return reprojection_loss    def compute_losses(self, inputs, outputs, features=None, features_teacher=None, teacher_outputs=None):        """Compute the reprojection and smoothness losses for a minibatch        """        # for i in range(len(features)):        #     print(features[i].shape, features_teacher[i].shape)        losses = {}        total_loss = 0        for scale in self.opt.scales:            loss = 0            reprojection_losses = []            if self.opt.v1_multiscale:                source_scale = scale            else:                source_scale = 0            disp = outputs[("disp", scale)]            color = inputs[("color", 0, scale)]            target = inputs[("color", 0, source_scale)]            for frame_id in self.opt.frame_ids[1:]:                pred = outputs[("color", frame_id, scale)]                reprojection_losses.append(self.compute_reprojection_loss(pred, target))            reprojection_losses = torch.cat(reprojection_losses, 1)            if not self.opt.disable_automasking:                identity_reprojection_losses = []                for frame_id in self.opt.frame_ids[1:]:                    pred = inputs[("color", frame_id, source_scale)]                    identity_reprojection_losses.append(                        self.compute_reprojection_loss(pred, target))                identity_reprojection_losses = torch.cat(identity_reprojection_losses, 1)                if self.opt.avg_reprojection:                    identity_reprojection_loss = identity_reprojection_losses.mean(1, keepdim=True)                else:                    # save both images, and do min all at once below                    identity_reprojection_loss = identity_reprojection_losses            elif self.opt.predictive_mask:                # use the predicted mask                mask = outputs["predictive_mask"]["disp", scale]                if not self.opt.v1_multiscale:                    mask = F.interpolate(                        mask, [self.opt.height, self.opt.width],                        mode="bilinear", align_corners=False)                reprojection_losses *= mask                # add a loss pushing mask to 1 (using nn.BCELoss for stability)                weighting_loss = 0.2 * nn.BCELoss()(mask, torch.ones(mask.shape).cuda())                loss += weighting_loss.mean()            if self.opt.avg_reprojection:                reprojection_loss = reprojection_losses.mean(1, keepdim=True)            else:                reprojection_loss = reprojection_losses            if not self.opt.disable_automasking:                # add random numbers to break ties                identity_reprojection_loss += torch.randn(                    identity_reprojection_loss.shape, device=self.device) * 0.00001                combined = torch.cat((identity_reprojection_loss, reprojection_loss), dim=1)            else:                combined = reprojection_loss            if combined.shape[1] == 1:                to_optimise = combined            else:                to_optimise, idxs = torch.min(combined, dim=1)            if not self.opt.disable_automasking:                outputs["identity_selection/{}".format(scale)] = (                    idxs > identity_reprojection_loss.shape[1] - 1).float()            loss += to_optimise.mean()            mean_disp = disp.mean(2, True).mean(3, True)            norm_disp = disp / (mean_disp + 1e-7)            smooth_loss = get_smooth_loss(norm_disp, color)            loss += self.opt.disparity_smoothness * smooth_loss / (2 ** scale)            # if self.epoch < 100:            #     loss = 0            # loss *= 5            # loss = 0            total_loss += loss            losses["loss/{}".format(scale)] = loss        if features and features_teacher is not None:            features_new = self.cat(features_teacher, features)            # T = 5.            T = 5.            distill_loss_kl = 0            distill_loss_cat = 0            total_loss /= self.num_scales            # """            # if self.epoch < 10:            if True:                for i in range(len(features_teacher)):                    # s = features[i].mean(dim=(2, 3), keepdim=False)                    # t = features_teacher[i].mean(dim=(2, 3), keepdim=False)                    # distill_loss_cat += torch.mean(torch.pow(s - t, 2))                    # s = features_new[i].mean(dim=(2, 3), keepdim=False)                    # t = features_teacher[i].mean(dim=(2, 3), keepdim=False)                    # distill_loss_kl += torch.mean(torch.pow(s - t, 2))                    # distill_loss_kl += F.kl_div(                    #     F.log_softmax(features[i] / T, dim = -1),                    #     F.softmax(features_teacher[i] / T, dim = -1),                    #     reduction = 'batchmean')                    # print(features_teacher[i].shape, features_new[i].shape)                    # distill_loss += F.mse_loss(                    #     features_new[i],                    #     features_teacher[i]                    # )                    distill_loss_cat += torch.mean(                        torch.pow(features_new[i] - features_teacher[i], 2)                    )                    # distill_loss_cat += self.feature_loss_function(features[i], features_teacher[i].detach())                    # distill_loss_cat += F.kl_div(                    #     F.log_softmax(features_new[i] / T, dim = -1),                    #     F.softmax(features_teacher[i] / T, dim = -1).detach(),                    #     reduction = 'batchmean')                # distill_loss_cat /= len(features_teacher)                distill_loss_cat = distill_loss_cat*0.1                # distill_loss = distill_loss_kl*(T**2) + 0.001*distill_loss_cat                # distill_loss /= len(features_teacher)                # distill_loss = (distill_loss)*0.7**self.epoch                                if self.epoch>=100:                    distill_loss_cat=distill_loss_cat*0.4**self.epoch                losses["loss/kl"] = distill_loss_cat                # losses["loss/cat"] = distill_loss_cat                # total_loss += distill_loss                total_loss += distill_loss_cat                losses["loss"] = total_loss            # """            if teacher_outputs is not None:                distill_loss_out = 0                sloss = nn.SmoothL1Loss()                for scale in self.opt.scales:                    disp_s = outputs[("disp", scale)]                    disp_t = teacher_outputs[("disp", scale)]                    _, depth_s = disp_to_depth(disp_s, self.opt.min_depth, self.opt.max_depth)                    _, depth_t = disp_to_depth(disp_t, self.opt.min_depth, self.opt.max_depth)                    # print(disp_s.shape, disp_t.shape)                    distill_loss_out += sloss(disp_s, disp_t).mean()                    # distill_loss_out += torch.log(torch.abs(disp_s - disp_t) + 0.5).mean()                #                # distill_loss_out /= len(self.opt.scales)                distill_loss_out = distill_loss_out*5                if self.epoch>=100:                    distill_loss_out=distill_loss_out*0.4**self.epoch                losses["loss/out"] = distill_loss_out                total_loss += distill_loss_out                losses["loss"] = total_loss            return losses        total_loss /= self.num_scales        losses["loss"] = total_loss        return losses    def compute_depth_losses(self, inputs, outputs, losses):        """Compute depth metrics, to allow monitoring during training        This isn't particularly accurate as it averages over the entire batch,        so is only used to give an indication of validation performance        """        depth_pred = outputs[("depth", 0, 0)]        depth_pred = torch.clamp(F.interpolate(            depth_pred, [375, 1242], mode="bilinear", align_corners=False), 1e-3, 80)        depth_pred = depth_pred.detach()        depth_gt = inputs["depth_gt"]        mask = depth_gt > 0        # garg/eigen crop        crop_mask = torch.zeros_like(mask)        crop_mask[:, :, 153:371, 44:1197] = 1        mask = mask * crop_mask        depth_gt = depth_gt[mask]        depth_pred = depth_pred[mask]        depth_pred *= torch.median(depth_gt) / torch.median(depth_pred)        depth_pred = torch.clamp(depth_pred, min=1e-3, max=80)        depth_errors = compute_depth_errors(depth_gt, depth_pred)        for i, metric in enumerate(self.depth_metric_names):            losses[metric] = np.array(depth_errors[i].cpu())    def log_time(self, batch_idx, duration, loss):        """Print a logging statement to the terminal        """        samples_per_sec = self.opt.batch_size / duration        time_sofar = time.time() - self.start_time        training_time_left = (            self.num_total_steps / self.step - 1.0) * time_sofar if self.step > 0 else 0        if len(self.opt.distill) > 0:            print_string = "epoch {:>3} | lr {:.6f} |lr_p {:.6f} |lr_m {:.6f} | batch {:>6} | examples/s: {:5.1f}" + \                           " | loss: {:.5f} | time elapsed: {} | time left: {}"            print(print_string.format(self.epoch, self.model_optimizer.state_dict()['param_groups'][0]['lr'],                                  self.model_pose_optimizer.state_dict()['param_groups'][0]['lr'],                                  self.model_mlp_optimizer.state_dict()['param_groups'][0]['lr'],                                  batch_idx, samples_per_sec, loss,                                  sec_to_hm_str(time_sofar), sec_to_hm_str(training_time_left)))        else:            print_string = "epoch {:>3} | lr {:.6f} |lr_p {:.6f} | batch {:>6} | examples/s: {:5.1f}" + \                           " | loss: {:.5f} | time elapsed: {} | time left: {}"            print(print_string.format(self.epoch, self.model_optimizer.state_dict()['param_groups'][0]['lr'],                                      self.model_pose_optimizer.state_dict()['param_groups'][0]['lr'],                                      batch_idx, samples_per_sec, loss,                                      sec_to_hm_str(time_sofar), sec_to_hm_str(training_time_left)))    def log(self, mode, inputs, outputs, losses, teacher_outputs):        """Write an event to the tensorboard events file        """        writer = self.writers[mode]        for l, v in losses.items():            writer.add_scalar("{}".format(l), v, self.step)        for j in range(min(4, self.opt.batch_size)):  # write a maxmimum of four images            for s in self.opt.scales:                for frame_id in self.opt.frame_ids:                    writer.add_image(                        "color_{}_{}/{}".format(frame_id, s, j),                        inputs[("color", frame_id, s)][j].data, self.step)                    if s == 0 and frame_id != 0:                        writer.add_image(                            "color_pred_{}_{}/{}".format(frame_id, s, j),                            outputs[("color", frame_id, s)][j].data, self.step)                writer.add_image(                    "disp_{}/{}".format(s, j),                    normalize_image(outputs[("disp", s)][j]), self.step)                if len(self.opt.distill) > 0 and teacher_outputs is not None:                    writer.add_image(                        "disp_t_{}/{}".format(s, j),                        normalize_image(teacher_outputs[("disp", s)][j]), self.step)                if self.opt.predictive_mask:                    for f_idx, frame_id in enumerate(self.opt.frame_ids[1:]):                        writer.add_image(                            "predictive_mask_{}_{}/{}".format(frame_id, s, j),                            outputs["predictive_mask"][("disp", s)][j, f_idx][None, ...],                            self.step)                elif not self.opt.disable_automasking:                    writer.add_image(                        "automask_{}/{}".format(s, j),                        outputs["identity_selection/{}".format(s)][j][None, ...], self.step)    def save_opts(self):        """Save options to disk so we know what we ran this experiment with        """        models_dir = os.path.join(self.log_path, "models")        if not os.path.exists(models_dir):            os.makedirs(models_dir)        to_save = self.opt.__dict__.copy()        with open(os.path.join(models_dir, 'opt.json'), 'w') as f:            json.dump(to_save, f, indent=2)    def save_model(self):        """Save model weights to disk        """        save_folder = os.path.join(self.log_path, "models", "weights_{}".format(self.epoch))        if not os.path.exists(save_folder):            os.makedirs(save_folder)        for model_name, model in self.models.items():            save_path = os.path.join(save_folder, "{}.pth".format(model_name))            to_save = model.state_dict()            if model_name == 'encoder':                # save the sizes - these are needed at prediction time                to_save['height'] = self.opt.height                to_save['width'] = self.opt.width                to_save['use_stereo'] = self.opt.use_stereo            torch.save(to_save, save_path)        for model_name, model in self.models_pose.items():            save_path = os.path.join(save_folder, "{}.pth".format(model_name))            to_save = model.state_dict()            torch.save(to_save, save_path)        save_path = os.path.join(save_folder, "{}.pth".format("adam"))        torch.save(self.model_optimizer.state_dict(), save_path)        save_path = os.path.join(save_folder, "{}.pth".format("adam_pose"))        if self.use_pose_net:            torch.save(self.model_pose_optimizer.state_dict(), save_path)    def load_pretrain(self):        self.opt.mypretrain = os.path.expanduser(self.opt.mypretrain)        path = self.opt.mypretrain        model_dict = self.models["encoder"].state_dict()        a = model_dict['stages.0.1.bn1.weight']        pretrained_dict = torch.load(path)['model']        # print(type(pretrained_dict))        # print(pretrained_dict['model'].keys())        # pd = {}        # for k, v in pretrained_dict.items():        #     if k == 'model':        #         print(v)        #     if k in model_dict:        #         print(k)        # input()        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if (k in model_dict and not k.startswith('norm')        #                                                                 and not k.startswith('downsample_layers.0')        #                                                                 and not k.startswith('downsample_layers.2')        #        #                                                                 and not k.startswith('stem2'))}        #        pretrained_dict = {k: v for k, v in pretrained_dict.items() if (k in model_dict and not k.startswith('norm')                                                                        )}        # input(pretrained_dict.keys())        model_dict.update(pretrained_dict)        # input(a==model_dict['stages.0.1.bn1.weight'])        self.models["encoder"].load_state_dict(model_dict)        # input(a == self.models["encoder"].state_dict())        print('mypretrain loaded.')    def load_model(self):        """Load model(s) from disk        """        self.opt.load_weights_folder = os.path.expanduser(self.opt.load_weights_folder)        assert os.path.isdir(self.opt.load_weights_folder), \            "Cannot find folder {}".format(self.opt.load_weights_folder)        print("loading model from folder {}".format(self.opt.load_weights_folder))        if len(self.opt.distill) > 0:            assert len(self.opt.distill) == len(self.opt.dis_loss)            encoder_path = os.path.join(self.opt.load_weights_folder, "encoder.pth")            depth_decoder_path = os.path.join(self.opt.load_weights_folder, "depth.pth")            loaded_dict_enc = torch.load(encoder_path, map_location=torch.device("cuda"))            loaded_dict_depth = torch.load(depth_decoder_path, map_location=torch.device("cuda"))            # extract the height and width of image that this model was trained with            feed_height = loaded_dict_enc['height']            feed_width = loaded_dict_enc['width']            filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in self.teacher.state_dict()}            filtered_dict_dec = {k: v for k, v in loaded_dict_depth.items() if k in self.teacher_depth.state_dict()}            self.teacher.load_state_dict(filtered_dict_enc)            self.teacher_depth.load_state_dict(filtered_dict_dec)            print("successfully load teacher weights.")            return        for n in self.opt.models_to_load:            print("Loading {} weights...".format(n))            path = os.path.join(self.opt.load_weights_folder, "{}.pth".format(n))            # print(n)            if n in ['pose_encoder','pose']:                model_dict = self.models_pose[n].state_dict()                pretrained_dict = torch.load(path)                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}                model_dict.update(pretrained_dict)                self.models_pose[n].load_state_dict(model_dict)            else:                model_dict = self.models[n].state_dict()                pretrained_dict = torch.load(path)                pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}                model_dict.update(pretrained_dict)                self.models[n].load_state_dict(model_dict)        return        # loading adam state        optimizer_load_path = os.path.join(self.opt.load_weights_folder, "adam.pth")        optimizer_pose_load_path = os.path.join(self.opt.load_weights_folder, "adam_pose.pth")        if os.path.isfile(optimizer_load_path):            print("Loading Adam weights")            optimizer_dict = torch.load(optimizer_load_path)            optimizer_pose_dict = torch.load(optimizer_pose_load_path)            self.model_optimizer.load_state_dict(optimizer_dict)            self.model_pose_optimizer.load_state_dict(optimizer_pose_dict)        else:            print("Cannot find Adam weights so Adam is randomly initialized")